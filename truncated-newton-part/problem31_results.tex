The objective function for the Broyden tridiagonal function, formulated as a nonlinear least-squares problem, is given by:
$$ F(x) = \frac{1}{2} \sum_{k=1}^{n} f_k(x)^2 $$
where the residual functions $f_k(x)$ are defined as:
$$ f_k(x) = (3 - 2x_k)x_k - x_{k-1} - 2x_{k+1} + 1 $$
This function is subject to the following boundary conditions:
$$ x_0 = 0 \quad \text{and} \quad x_{n+1} = 0 $$
The standard starting point for optimization algorithms is $x_i = -1$ for all $i=1, \dots, n$.

\subsubsection*{Gradient of the Objective Function}

The gradient of the objective function is required for derivative-based optimization methods. The factor of $\frac{1}{2}$ in the objective function simplifies the expression.
$$ \frac{\partial F}{\partial x_j} = \sum_{k=1}^{n} f_k(x) \frac{\partial f_k}{\partial x_j} $$
Using the Kronecker delta, $\delta_{ij}$, the partial derivative of the residual function $f_k(x)$ with respect to $x_j$ is:
$$ \frac{\partial f_k}{\partial x_j} = (3 - 4x_k)\delta_{kj} - \delta_{k,j+1} - 2\delta_{k,j-1} $$
Substituting this into the gradient expression and simplifying the sum by using the sifting property of the Kronecker delta, we arrive at a single compact formula. By defining $f_0 = 0$ and $f_{n+1} = 0$ to handle the boundaries, the gradient components are:
$$ \frac{\partial F}{\partial x_j} = f_j(x) (3 - 4x_j) - f_{j+1}(x) - 2 f_{j-1}(x) $$
This single expression is valid for all $j=1, \dots, n$.

\subsubsection*{Hessian of the Objective Function}

For second-order optimization methods, the Hessian of $F(x) = \frac{1}{2}\mathbf{f}(x)^T \mathbf{f}(x)$ is given by:
$$ \nabla^2 F(x) = J(x)^T J(x) + \sum_{k=1}^{n} f_k(x) \nabla^2 f_k(x) $$
where $J(x)$ is the Jacobian matrix of the vector of residuals $\mathbf{f}(x) = [f_1(x), \dots, f_n(x)]^T$.

The Jacobian matrix, $J(x)$, has a tridiagonal structure based on the partial derivatives of the residuals:
$$ J(x) = \begin{pmatrix}
3-4x_1 & -2 & 0 & \dots & 0 \\
-1 & 3-4x_2 & -2 & \dots & 0 \\
0 & -1 & 3-4x_3 & \dots & 0 \\
\vdots & \vdots & \ddots & \ddots & \vdots \\
0 & 0 & \dots & -1 & 3-4x_n
\end{pmatrix} $$
The second term involves the Hessians of the individual residual functions, $\nabla^2 f_k(x)$. The only non-zero second partial derivative of $f_k(x)$ is:
$$ \frac{\partial^2 f_k}{\partial x_k^2} = -4 $$
This means the summation term simplifies to a diagonal matrix:
$$ \sum_{k=1}^{n} f_k(x) \nabla^2 f_k(x) = \text{diag}(-4f_1(x), -4f_2(x), \dots, -4f_n(x)) $$
Combining the terms, the full Hessian of the objective function is:
$$ \nabla^2 F(x) = J(x)^T J(x) - 4 \cdot \text{diag}(f_1(x), f_2(x), \dots, f_n(x)) $$